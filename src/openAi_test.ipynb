{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed # for exponential backoff\n",
    "\n",
    "# init openAI client. You need to set your openAI API key in your environment variables to make this work\n",
    "client = OpenAI()\n",
    "\n",
    "# GPT version which should be used\n",
    "MODEL_ID = \"gpt-3.5\"\n",
    "# system prompt for the GPT model\n",
    "SYSTEM_PROMPT = \"you are an expert on clinical trials in the medical domain. You will get up to two pieces of evidence and a statement. Your job is to decide if the statement makes logical sense, given the pieces of evidence.\"\n",
    "# location of the preprocessed dataset\n",
    "DATA_LOCATION = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_dataset('json', data_files={'train': [f'{DATA_LOCATION}train.json'],\n",
    "                                           'val': [f'{DATA_LOCATION}val.json'],\n",
    "                                         'test': [f'{DATA_LOCATION}test.json']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train + val data, so we can explore both together\n",
    "# test with only 100, so it does not get too expensive\n",
    "val_data = data['val'].to_pandas()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retry decorator to avoid hitting usage limits\n",
    "@retry(wait=wait_fixed(10), stop=stop_after_attempt(6))\n",
    "def sendPrompt(prompt: str, model_id):\n",
    "    \"\"\"Sends a prompt to a GPT model via the OpenAI API\n",
    "\n",
    "    Args:\n",
    "        prompt: prompt to send to the model\n",
    "        model_id: OpenAI model ID (e.g. gpt-3.5)\n",
    "\n",
    "    Returns:\n",
    "        response of the model\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    #print(completion.choices[0].message.content)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def label_to_id(label: str):\n",
    "    \"\"\"Converts the output label of the model to ints, so we can analyze the performance.\n",
    "    Entailment is converted to 0, contradiction is converted to 1\n",
    "\n",
    "    Args:\n",
    "        label (str): the label to be converted\n",
    "\n",
    "    Returns:\n",
    "        the int label\n",
    "    \"\"\"    \n",
    "    pred = label.strip().lower()\n",
    "    # sometimes the models reply yes/no -> translate this to entailment/contradiction\n",
    "    if pred == \"entailment\" or pred == \"yes\":\n",
    "        return 1\n",
    "    if pred == \"contradiction\" or pred == \"no\":\n",
    "        return 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "# Create an empty list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the rows in val_data with tqdm for progress bar\n",
    "for index, row in tqdm(val_data.iterrows(), total=len(val_data)):\n",
    "    # Call sendPrompt function and append the prediction to the list\n",
    "    predictions.append(sendPrompt(row['text'], MODEL_ID))\n",
    "val_data['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    row['prediction'] = row['prediction'].apply(label_to_id)\n",
    "    row['label'] = row['label'].apply(label_to_id)\n",
    "    \n",
    "    return f1_score(row['label'],row['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8823529411764707"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "gpt3.5-turbo: 0.6629834254143647 (costs 0.16 $ for 200 examples)\n",
    "\n",
    "gpt4: 0.8823529411764707 (costs 2.10 $ for 100 examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
